#-m -p hydra.sweeper

defaults:
  - hydra/job_logging: disabled
  - hydra/output_subdir: null
#  - hydra/sweeper: ax


main:
    experiment_name_prefix: "my_exp"
    seed: 1
    num_workers: 6
    trains: False
    paths:
      train: '/home/student/data/train' # '/home/student/hw2/data/train_loader.pkl'
      test: '/home/student/data/test' # '/home/student/hw2/data/val_loader.pkl'
      example: './data/example' # '/home/student/hw2/data/train_dataset.pth'
      logs: '/home/student/inbals_sync/saved_models'
    model_names:
      q_model_name: 'attention_lstm' #'lstm' #
      v_model_name: 'attention_cnn' #'cnn' #
      vqa_model_name: 'atten_lstm_cnn' #'basic_lstm_cnn' #

train:
    num_epochs: 15
    grad_clip: 0.25
#    dropout: 0.3
#    num_hid: 20
    batch_size: 16 # 16 # 10
    save_model: False
    lr:
      lr_value: 1e-3 #0.1 #1e-2
      lr_decay: 15
      lr_gamma: 0.1
      lr_step_size: 3.0 # 0.5 #3.0 # 30.0

dataset:
  resize_h: 224 #365 #640
  resize_w: 224 #365 #640


class_model:
  cnn:
    dims: [3, 32, 64, 128] # [3, 32, 64]
    fc_out: 1
    is_atten: False
    is_autoencoder: False

bbox_model:
  cnn:
    dims: [3, 32, 64, 128] # [3, 32, 64]
    fc_out: 4
    is_atten: False
    is_autoencoder: False
v_model:
  cnn:
    dims: [3, 32, 64, 128] # [3, 32, 64]
    kernel_size: 3 # 5
    padding: 1 # 2
    pool: 2
    activation: 'ReLU'

  attention_cnn:
    dims: [3, 32, 32, 64, 64, 128, 128, 256, 256] # [3, 32, 64, 128, 256]  # # [3, 16, 32, 64, 128 ,256, 512, 1024] #
    kernel_size: 3 #5
    padding: 1 #2
    pool: 2
    fc_out: -1 # should be equal to fc_in that is calculated in the model init
    activation: 'ReLU'
    is_atten: True
    is_autoencoder: False

atten_model:
  projected_dim: 500

vqa_model:
  basic:
    activation: 'ReLU'
    num_hid: 2048
    dropout: 0.3
    is_concat: True
  basic_lstm_cnn:
    activation: 'ReLU'
    num_hid: 2048
    dropout: 0.3
    is_concat: True
  atten_lstm_cnn:
    activation: 'ReLU'
    num_hid: 2048
    dropout: 0.3
    is_concat: False # True
data:
  img_size: 224

#hydra:
#    output_subdir: null
#    run:
#      dir: logs/hydra
#    sweeper:
#      # The following part of config is used to setup the Hydra Ax plugin and is optional
#      ax_config:
#        # max_trials is application-specific. Tune it for your use case
#        max_trials: 20
#
#        experiment:
#          # Default to minimize, set to false to maximize
#          minimize: False
#
#        early_stop:
#          # Number of epochs without a significant improvement from
#          # the currently known best parameters
#          # An Epoch is defined as a batch of trials executed in parallel
#          max_epochs_without_improvement: 20
#
#        params:
#          train.lr.lr_step_size:
#            type: choice
#            values: [3.0, 30.0, 0.5]
#            value_type: float
#          q_model.lstm.emb_dim:
#            type: choice
#            values: [100, 300]
#            value_type: int
#          q_model.lstm.num_layer:
#            type: choice
#            values: [1, 2]
#            value_type: int
#          q_model.attention_lstm.emb_dim:
#            type: choice
#            values: [ 100, 300 ]
#            value_type: int
#          q_model.attention_lstm.num_layer:
#            type: choice
#            values: [ 1, 2 ]
#            value_type: int
#          v_model.cnn.dims:
#            type: choice
#            values: [
#                '[3, 16, 32, 64]',
#                '[3, 64, 128, 256]',
#                '[3, 16, 64]',
#                '[3, 16, 64, 128, 256, 512]',
#            ]
#            value_type: str
#          v_model.attention_cnn.dims:
#            type: choice
#            values: ['[3, 16, 32, 64]',
#                     '[3, 64, 128, 256]',
#                     '[3, 16, 64]',
#                     '[3, 16, 64, 128, 256, 512]',
#            ]
#            value_type: str
#          v_model.cnn.kernel_size:
#            type: choice
#            values: [3, 5]
#            value_type: int
#          v_model.attention_cnn.kernel_size:
#            type: choice
#            values: [3, 5]
#            value_type: int
#          atten_model.projected_dim:
#            type: choice
#            values: [200, 1024]
#            value_type: int
#          vqa_model.basic_lstm_cnn.is_concat:
#            type: choice
#            values: [ False, True ]
#            value_type: bool
#          vqa_model.atten_lstm_cnn.is_concat:
#            type: choice
#            values: [ False, True ]
#            value_type: bool



